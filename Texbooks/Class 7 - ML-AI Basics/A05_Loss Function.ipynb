{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Loss Function"],"metadata":{"id":"9L632FHJC5HR"}},{"cell_type":"markdown","source":["## Introduction"],"metadata":{"id":"wOtB_UPX8H_a"}},{"cell_type":"markdown","source":["在機器學習和深度學習中，損失函數（或稱為成本函數）是一種衡量模型預測結果與實際數據之間差異的函數。它是訓練過程中優化的核心，其目的是通過調整模型參數來最小化這一函數，從而使模型的預測盡可能接近真實數據。\n","\n"],"metadata":{"id":"EbM9mMx4L6y7"}},{"cell_type":"markdown","source":["目標函數基本上都是「損失函數(loss function)」，而模型的好壞有絕大部分的因素來至損失函數的設計。損失函數基本上可以分成兩個面向(分類和回歸)，基本上都是希望最小化損失函數。常見的損失函數有Entropy、Cross Entropy、KL Divergence 、f-Divergence、MSE、MAE。"],"metadata":{"id":"aj4-qyKWsqsx"}},{"cell_type":"markdown","source":["## Loss Function 有哪些？"],"metadata":{"id":"QW23ckAo_JmN"}},{"cell_type":"markdown","source":["### 回歸型 Loss function"],"metadata":{"id":"dWPcLJmKxuzy"}},{"cell_type":"markdown","source":["回歸損失函數用於衡量連續型變數的預測誤差。\n","\n","1. **均方誤差（Mean Squared Error, MSE）**：\n","   $$\n","   MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","   $$\n","   - 用於衡量預測值與實際值之間的平方差的平均值。\n","\n","2. **均方根誤差（Root Mean Squared Error, RMSE）**：\n","   $$\n","   RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n","   $$\n","   - 是MSE的平方根，更直觀地表示誤差的大小。\n","\n","3. **平均絕對誤差（Mean Absolute Error, MAE）**：\n","   $$\n","   MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n","   $$\n","   - 衡量預測值與實際值之間的絕對差的平均值，不易受異常值影響。\n","\n","4. **Huber損失（Huber Loss）**：\n","   - 結合了MSE和MAE的優點，對於小誤差使用MSE，對於大誤差使用MAE，公式如下：\n","\n","     Huber(y, \\hat{y}) =\n","     \\begin{cases}\n","     \\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n","     \\delta |y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise}\n","     \\end{cases}\n"],"metadata":{"id":"C-Q-mGH1yQLr"}},{"cell_type":"markdown","source":["MSE（均方誤差）\n","\n","優點：\n","- MSE對大誤差的懲罰比MAE更嚴重。如果您關注數據中的異常值，這點非常重要。\n","- MSE是一個可微分的函數，使其更容易與基於梯度的優化算法一起使用。\n","\n","缺點：\n","- MSE對異常值更敏感。如果數據中有很多異常值，這會成為一個問題。\n","- 如果數據偏態，MSE可能會誤導。因為MSE給予較大誤差更多的權重，這可能會使結果偏斜。\n","\n","MAE（平均絕對誤差）\n","\n","優點：\n","- MAE對異常值的敏感度比MSE低。如果數據中有很多異常值，這點非常重要。\n","- MAE不受數據偏態的影響。\n","\n","缺點：\n","- MAE對大誤差的懲罰不如MSE嚴重。如果您關注數據中的異常值，這會成為一個問題。\n","- MAE不是可微分的函數，這使得它與基於梯度的優化算法一起使用時更困難。\n","\n","選擇誤差指標時還需考慮一些其他因素：\n","\n","- 可解釋性：MAE通常比MSE更容易解釋，因為它表示了預測值和實際值之間的平均絕對差異。\n","- 魯棒性：MAE比MSE對異常值更具魯棒性。\n","- 計算效率：MSE在計算上比MAE更高效。"],"metadata":{"id":"-43oiott3zo8"}},{"cell_type":"markdown","source":["### 分類型 Loss function"],"metadata":{"id":"nyr1_8ziyE5U"}},{"cell_type":"markdown","source":["當我們處理分類問題時，損失函數是用來衡量模型預測和實際標籤之間差異的關鍵工具。以下是一些常見的分類型損失函數的詳細介紹：\n","\n","1. 二元交叉熵損失（Binary Cross-Entropy Loss）\n","\n","**定義**：\n","二元交叉熵損失（也稱為對數損失或log損失）主要用於二分類問題。它衡量的是預測的概率分佈與真實標籤之間的差異。\n","\n","$$\n","H(p, q) = - \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n","$$\n","\n","其中：\n","- \\( y \\) 是真實標籤（0或1）。\n","- \\( \\hat{y} \\) 是模型預測的概率。\n","\n","**特點**：\n","- 當預測的概率接近真實標籤時，損失會很小。\n","- 當預測的概率與真實標籤相差很大時，損失會很大。\n","- 常用於二元分類問題，如垃圾郵件檢測、疾病診斷等。\n","\n","2. 多元交叉熵損失（Categorical Cross-Entropy Loss）\n","\n","**定義**：\n","多元交叉熵損失用於多分類問題，衡量多個類別之間的預測概率分佈與真實標籤之間的差異。\n","\n","對於單個樣本，損失函數為：\n","$$\n","H(p, q) = - \\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n","$$\n","\n","其中：\n","- \\( C \\) 是類別數。\n","- \\( y_i \\) 是真實標籤的one-hot編碼。\n","- \\( \\hat{y}_i \\) 是模型對類別 \\( i \\) 的預測概率。\n","\n","**特點**：\n","- 能夠處理多個類別的分類問題，如圖片分類（貓、狗、鳥等）。\n","- 預測的概率越接近真實標籤，損失越小。\n","\n","3. 稀疏多元交叉熵損失（Sparse Categorical Cross-Entropy Loss）\n","\n","**定義**：\n","稀疏多元交叉熵損失與多元交叉熵損失類似，但它不需要對真實標籤進行one-hot編碼。適用於類別數量很多但只有少量類別有標記的情況。\n","\n","**特點**：\n","- 節省內存，適用於類別數量很大但標記稀疏的情況。\n","- 損失計算更簡單，適合大型數據集。\n","\n","4. Hinge損失（Hinge Loss）\n","\n","**定義**：\n","Hinge損失主要用於支持向量機（SVM），衡量模型預測與真實標籤之間的距離。\n","\n","$$\n","H(y, \\hat{y}) = \\max(0, 1 - y \\hat{y})\n","$$\n","\n","其中：\n","- \\( y \\) 是真實標籤（+1或-1）。\n","- \\( \\hat{y} \\) 是模型的預測值。\n","\n","**特點**：\n","- 當預測值與真實標籤的乘積大於1時，損失為0。\n","- 當預測值與真實標籤的乘積小於1時，損失會增加。\n","- 用於SVM模型的訓練，確保預測值與真實標籤的距離足夠大。\n","\n","5. Kullback-Leibler散度（KL Divergence）\n","\n","**定義**：\n","KL散度衡量兩個概率分佈之間的差異。它在生成模型和概率模型中非常常用。\n","\n","$$\n","D_{KL}(P || Q) = \\sum_{x} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right)\n","$$\n","\n","**特點**：\n","- KL散度非對稱，即 \\( D_{KL}(P || Q) \\neq D_{KL}(Q || P) \\)。\n","- 在生成對抗網絡（GANs）和變分自編碼器（VAE）中廣泛使用，用於衡量生成樣本與真實樣本的差異。\n","\n","6. Jensen-Shannon散度（Jensen-Shannon Divergence, JS Divergence）\n","\n","**定義**：\n","JS散度是一種對稱的散度度量，用於衡量兩個概率分佈之間的差異。\n","\n","$$\n","D_{JS}(P || Q) = \\frac{1}{2} D_{KL}(P || M) + \\frac{1}{2} D_{KL}(Q || M)\n","$$\n","\n","其中 \\( M = \\frac{1}{2}(P + Q) \\)。\n","\n","**特點**：\n","- 對稱性，即 \\( D_{JS}(P || Q) = D_{JS}(Q || P) \\)。\n","- 更平滑、更穩定，常用於生成對抗網絡（GANs）中。\n","\n"],"metadata":{"id":"VNesaYSRymQn"}},{"cell_type":"markdown","source":["# References"],"metadata":{"id":"2hZmnvjiqhls"}}]}