{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(\"/Users/wenye/ML&AI Tutor/.env.local\")\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_KEY)\n",
    "OPENAI_EMBEDDING = OpenAIEmbeddings(api_key=OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方介紹：https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain Expression Language (LCEL) 簡介**\n",
    "\n",
    "**概述：**\n",
    "LCEL (LangChain Expression Language) 是一種用於在 LangChain 中構建 Runnables 的聲明式框架，允許開發者描述所需結果而非具體的執行細節。這種方式有助於在執行“chains”時進行優化，這些“chains”是由多個 Runnables 組成，並實現了 Runnable Interface。\n",
    "\n",
    "**LCEL 的優點：**\n",
    "- **平行執行**：透過 `RunnableParallel` 或 Batch API 支持並行運行，以減少延遲。\n",
    "- **非同步支持**：通過 Runnable Async API 提供非同步運行，適合在伺服器環境中處理大量請求。\n",
    "- **串流**：允許鏈式執行過程中逐步輸出，減少**time-to-first-token**（首次輸出延遲）。\n",
    "- **LangSmith Tracing**：自動記錄所有步驟以提高觀察性和除錯能力。\n",
    "- **標準化 API**：所有 chains 都使用統一的 Runnable 接口，可像其他 Runnables 一樣使用。\n",
    "- **部署支持**：可透過 LangServe 進行生產部署。\n",
    "\n",
    "**應用 LCEL 的時機：**\n",
    "- 適合較簡單的協作任務（如 prompt + LLM + 解析器）。\n",
    "- 對於包含分支、迴圈或多個代理的複雜應用，應使用 LangGraph，同時可在節點內整合 LCEL。\n",
    "\n",
    "**組合基元：**\n",
    "- **RunnableSequence**：將多個 Runnables 按序連接，一個的輸出作為下一個的輸入。\n",
    "- **RunnableParallel**：並行運行多個 Runnables，使用相同的輸入，返回包含各輸出結果的字典。\n",
    "\n",
    "**組合語法：**\n",
    "- **`|` 運算子**：用於創建 `RunnableSequence`，使代碼更易讀。\n",
    "- **`.pipe()` 方法**：`|` 運算子的替代方案。\n",
    "- **類型轉換**：自動將函數和字典轉換為 `RunnableLambda` 和 `RunnableParallel`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "output1 = runnable1.invoke(some_input)\n",
    "final_output = runnable2.invoke(output1)\n",
    "```\n",
    "\n",
    "上面的程式碼是一個最簡單的 sequence 寫法，然而可以用套件包裝：\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence([runnable1, runnable2])\n",
    "final_output = chain.invoke(some_input)\n",
    "```\n",
    "\n",
    "可以用 LCEL 語法：\n",
    "\n",
    "```python\n",
    "chain = runnable1 | runnable2\n",
    "```\n",
    "\n",
    "用 .pipe() 的寫法：\n",
    "\n",
    "```python\n",
    "chain = runnable1.pipe(runnable2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why do bears have sticky fur? \\n\\nBecause they always use honeycombs!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 13, 'total_tokens': 29, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-4e8b1e7e-127a-4f3d-8068-432d6efe8883-0', usage_metadata={'input_tokens': 13, 'output_tokens': 16, 'total_tokens': 29, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 LCEL 語法建構\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "# 這裡是一個簡單的串接，將 prompt -> llm -> StrOutputParser 串接起來\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do bears have hairy coats?\\n\\nBecause they look silly in sweaters!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整的流程是這樣的：\n",
    "1. 輸入是一個 dict，裡面有一個 key 是 \"topic\"，value 是 \"bears\"\n",
    "2. 這個 topic 會輸入到 prompt 中的 {topic} 裡面，成為一個完整的 prompt，然後送到 llm 中進行問答\n",
    "3. 接著，LLM 會回傳一個 response，這個 response 會被 StrOutputParser 解析成一個字串，然後輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a cute joke! It's lighthearted and has a playful twist. Humor can be subjective, but many people might find the image of a bear in a jacket amusing. If you like it, keep sharing it!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 試著分析看看這一個 Runable Chain!\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "\n",
    "composed_chain = {\"joke\": chain} | analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "composed_chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡打下你的流程：\n",
    "\n",
    "1. topic 中的值會先進入到 chain 中，經過 prompt, StrOutputParser 「依序」處理之後，產生一個結果，先定義成 joke_value\n",
    "2. 接著回到 composed_chain, 會變成 {\"joke\": joke_value} 的資料\n",
    "3. 送進去 analysis_prompt 中，變成完整的 prompt\n",
    "4. 送進去 LLM 中，最後經過 StrOutputParser 得到完整的結果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That\\'s a clever play on words! The joke combines a reference to the Cylon characters from \"Battlestar Galactica,\" who are known for their artificial intelligence, with the common theme of seeking a \"human connection\" in relationships. It’s a nice blend of sci-fi humor and relationship commentary. If your audience is familiar with the show, they’re likely to appreciate it!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以用 pipe 的方式串接\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "composed_chain_with_pipe = (\n",
    "    RunnableParallel({\"joke\": chain})\n",
    "    .pipe(analysis_prompt)\n",
    "    .pipe(llm)\n",
    "    .pipe(StrOutputParser())\n",
    ")\n",
    "\n",
    "composed_chain_with_pipe.invoke({\"topic\": \"battlestar galactica\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Why| did| the| par|rot| wear| a| rain|coat|?\n",
      "\n",
      "|Because| it| wanted| to| be| a| poly|uns|aturated|!||"
     ]
    }
   ],
   "source": [
    "# 當然，可以 invoke 就可以 stream 顯示！\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel 是用於需要同步進行兩個 Runnable 時使用。\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "final_output = {\n",
    "    \"key1\": runnable1.invoke(some_input),\n",
    "    \"key2\": runnable2.invoke(some_input),\n",
    "}\n",
    "```\n",
    "\n",
    "上面的程式碼是一個最簡單的 Parallel 寫法，然而可以用套件包裝：\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "chain = RunnableParallel({\n",
    "    \"key1\": runnable1,\n",
    "    \"key2\": runnable2,\n",
    "})\n",
    "final_output = chain.invoke(some_input)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=OPENAI_EMBEDDING\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# The prompt expects input with keys for \"context\" and \"question\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# 在這裡，我們要同時處理 context 和 question，所以我們使用 RunnableParallel\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html\n",
    "\n",
    "功能概述： RunnablePassthrough 是一個將輸入原封不動地傳遞出去或附加額外鍵值的 Runnable。它的行為類似於「恆等函數」，但如果輸入為字典，則可以設定為在輸出中添加額外的鍵值。\n",
    "\n",
    "應用場景： 這個 Runnable 適合用於需要保留原始輸入的情況，或者在傳遞輸入時希望附加其他信息的場景。此特性在簡單的執行鏈中非常有用，特別是那些用於實驗和測試的鏈。\n",
    "\n",
    "示例應用： 下列示例展示了 RunnablePassthrough 如何在簡單的鏈中運作。這些鏈使用簡單的 lambda 函數，使示例更易於執行和實驗。\n",
    "\n",
    "總結來說，RunnablePassthrough 是一個輕量級但靈活的工具，適合在保持輸入數據完整性的同時，對輸出進行輕微擴展。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"\\nQuestion: where did harrison work?\\nmemo: where did harrison work?\\ncontext: [Document(metadata={}, page_content='harrison worked at kensho')]\\n\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Question: {question}\n",
    "memo: {memo}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "retrieval_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"memo\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | prompt\n",
    "\n",
    "response = retrieval_chain.invoke(\"where did harrison work?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一個值傳給當下所有，但建議是用下面的方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"\\nQuestion: where did harrison work?\\nmemo: haa\\ncontext: [Document(metadata={}, page_content='harrison worked at kensho')]\\n\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "retrieval_chain = {\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"memo\": itemgetter(\"memo\"),\n",
    "    \"context\": itemgetter(\"question\") | retriever,\n",
    "} | prompt\n",
    "\n",
    "response = retrieval_chain.invoke(\n",
    "    {\"question\": \"where did harrison work?\", \"memo\": \"haa\"}\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'modified': 2}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = RunnableParallel(\n",
    "    passed=RunnablePassthrough(),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")  # 一次把所有的東西都傳給同一層所有的 runnables\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .bind()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在某些情況下，我們希望在 RunnableSequence 中調用某個 Runnable 並使用常數參數，而這些參數既不屬於序列中前一個 Runnable 的輸出，也不屬於用戶輸入。為此，可以使用 Runnable.bind() 方法提前設定這些參數。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION: \\( x^3 + 7 = 12 \\)\n",
      "\n",
      "SOLUTION:\n",
      "1. Subtract 7 from both sides:\n",
      "   \\[\n",
      "   x^3 = 12 - 7\n",
      "   \\]\n",
      "   \\[\n",
      "   x^3 = 5\n",
      "   \\]\n",
      "\n",
      "2. Take the cube root of both sides:\n",
      "   \\[\n",
      "   x = \\sqrt[3]{5}\n",
      "   \\]\n",
      "\n",
      "Thus, the solution is:\n",
      "\\[\n",
      "x \\approx 1.71\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n",
    "        ),\n",
    "        (\"human\", \"{equation_statement}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "runnable = (\n",
    "    {\"equation_statement\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子，可以看到完整的結果，但如果我只想要 EQUATION 的話呢？\n",
    "\n",
    "這時候會需要用到 openai model 中有提供看到 stop 的服務，可以看到特定字符就停止。\n",
    "\n",
    "這時候可以用 Runnable.bind() 來提前設定某些參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION: \\( x^3 + 7 = 12 \\)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runnable = (\n",
    "    {\"equation_statement\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm.bind(stop=\"SOLUTION\")  # 我只想要取到 EQUATION 的部分，所以終止符號是 SOLUTION\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION: \\( x^3 + 7 = 12 \\)\n",
      "\n",
      "SOLUTION: \n",
      "\n",
      "1. Subtract 7 from both sides:\n",
      "   \\[\n",
      "   x^3 = 12 - 7\n",
      "   \\]\n",
      "   \\[\n",
      "   x^3 = 5\n",
      "   \\]\n",
      "\n",
      "2. Take the cube root of both sides:\n",
      "   \\[\n",
      "   x = \\sqrt[3]{5}\n",
      "   \\]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runnable = (\n",
    "    {\"equation_statement\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm.bind(stop=\"Thus\")  # 我要取到 SOLUTION 的部分，所以終止符號是 Thus\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們有時候會需要在 Sequnce 與 Parallel 之中插入一些簡單的 function，\n",
    "\n",
    "可能是數學或是改變輸入格式。這時候就用到 RunnableLambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3 + 9 equals 12.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-648c1c13-9369-4dd9-bacc-7dad8cf639fd-0', usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),  # sub-chain\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
    "        | RunnableLambda(multiple_length_function),  # sub-chain\n",
    "    }  # This is a parallel chain\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why d'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在 LCEL 中，我們可以使用 lambda 來將 function 進行轉換。傳進去若沒有特別指定都是 x\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "\n",
    "chain_with_coerced_function = prompt | llm | (lambda x: x.content[:5])\n",
    "\n",
    "chain_with_coerced_function.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route the Branch(sub-chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "路由是一種用來創建非確定性鏈的方法，其中上一個步驟的輸出決定了下一個步驟的執行。透過使用路由，你可以為模型交互提供結構和一致性，這樣可以定義狀態，並使用與這些狀態相關的信息作為模型呼叫的上下文。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方建議使用 function 的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# 由 LLM 決定該問題是哪一個主題\n",
    "\n",
    "judge_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "    )\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# 我們假設有三條不同 Branch 的鏈，分別是 langchain_chain, anthropic_chain, general_chain\n",
    "# 各自進行不同的任務\n",
    "\n",
    "langchain_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in langchain. \\\n",
    "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(temperature=0.7, model=\"gpt-4o-mini\", api_key=OPENAI_KEY)\n",
    ")\n",
    "\n",
    "\n",
    "anthropic_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in anthropic. \\\n",
    "Always answer questions starting with \"As Dario Amodei told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(temperature=0.7, model=\"gpt-4o-mini\", api_key=OPENAI_KEY)\n",
    ")\n",
    "\n",
    "\n",
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(temperature=0.7, model=\"gpt-4o-mini\", api_key=OPENAI_KEY)\n",
    ")\n",
    "\n",
    "\n",
    "# function 用來判斷要使用哪一條鏈\n",
    "def route(info):\n",
    "    if \"anthropic\" in info[\"topic\"].lower():\n",
    "        return anthropic_chain\n",
    "    elif \"langchain\" in info[\"topic\"].lower():\n",
    "        return langchain_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain question: \n",
      "content='As Harrison Chase told me, to use LangChain, you generally start by installing the library and its dependencies. You can do this via pip with the command `pip install langchain`. After installation, you can create a chain by importing the necessary modules and defining your prompt templates, memory, and models. You’ll typically need to set up a language model, define how you want to process the input and output, and then run your chain to see the results. LangChain is designed to be flexible, allowing you to customize the components based on your specific use case. Don’t forget to check out the documentation for detailed examples and advanced features!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 49, 'total_tokens': 178, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None} id='run-dc190275-6a1c-4757-b41d-f78c3564ef4a-0' usage_metadata={'input_tokens': 49, 'output_tokens': 129, 'total_tokens': 178, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Anthropic question: \n",
      "content=\"As Dario Amodei told me, using Anthropic involves understanding the specific tools and models we offer. First, you'll want to familiarize yourself with our API, which allows you to integrate our models into your applications. You can start by signing up for access on our website and reviewing the documentation provided. This will guide you through the process of making API calls, handling responses, and leveraging our models for various tasks, such as natural language processing or conversational agents. Additionally, exploring our tutorials and community resources can greatly enhance your understanding and implementation of Anthropic's technology.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 114, 'prompt_tokens': 52, 'total_tokens': 166, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_9b78b61c52', 'finish_reason': 'stop', 'logprobs': None} id='run-81935971-d081-4ba8-bf5c-547c7826ac86-0' usage_metadata={'input_tokens': 52, 'output_tokens': 114, 'total_tokens': 166, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "General question: \n",
      "content='政治大學位於台灣的台北市。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 31, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None} id='run-35249c42-8290-47be-a518-80cd65ada3ef-0' usage_metadata={'input_tokens': 31, 'output_tokens': 12, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\"topic\": judge_chain, \"question\": RunnablePassthrough()} | RunnableLambda(\n",
    "    route\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Langchain question: \\n{full_chain.invoke({'question': 'how do I use LangChain?'})}\"\n",
    ")\n",
    "print(\n",
    "    f\"Anthropic question: \\n{full_chain.invoke({'question': 'how do I use Anthropic?'})}\"\n",
    ")\n",
    "print(\n",
    "    f\"General question: \\n{full_chain.invoke({'question': '政治大學在哪一個地區？'})}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a dynamic (self-constructing) chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有時候，我們希望在運行時根據鏈的輸入來構建鏈的部分結構（路由是這類情況中最常見的例子）。這可以透過 RunnableLambda 的一個非常有用的特性來實現：如果 RunnableLambda 返回一個 Runnable，那麼該 Runnable 會被自動調用。這使得動態構建鏈成為可能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As of 2024, Egypt's population is about 111 million.\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough, chain\n",
    "\n",
    "contextualize_instructions = \"\"\"Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text).\"\"\"\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_instructions),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_question = contextualize_prompt | llm | StrOutputParser()\n",
    "\n",
    "qa_instructions = (\n",
    "    \"\"\"Answer the user question given the following context:\\n\\n{context}.\"\"\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", qa_instructions), (\"human\", \"{question}\")]\n",
    ")\n",
    "\n",
    "\n",
    "# 這個 chain 會根據是否有 chat_history 來決定要不要 contextualize\n",
    "# 是一個 route, 如果有 chat_history 就會呼叫 contextualize_question 這個 runnable 進行問題改寫；否則則直接返回 question\n",
    "@chain\n",
    "def contextualize_if_needed(input_: dict) -> Runnable:\n",
    "    if input_.get(\"chat_history\"):\n",
    "        # NOTE: This is returning another Runnable, not an actual output.\n",
    "        return contextualize_question  # 這裡是一個額外的 Runnable\n",
    "    else:\n",
    "        return RunnablePassthrough() | itemgetter(\"question\")\n",
    "\n",
    "\n",
    "# 這個 chain 沒有什麼意義\n",
    "@chain\n",
    "def fake_retriever(input_: dict) -> str:\n",
    "    return \"egypt's population in 2024 is about 111 million\"\n",
    "\n",
    "\n",
    "full_chain = (\n",
    "    RunnablePassthrough.assign(question=contextualize_if_needed).assign(\n",
    "        context=fake_retriever\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "full_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what about egypt\",\n",
    "        \"chat_history\": [\n",
    "            (\"human\", \"what's the population of indonesia\"),\n",
    "            (\"ai\", \"about 276 million\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configurable_fields, configurable_alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能希望在執行鏈中進行實驗，甚至將多種方法公開給最終用戶使用。這可能包括調整參數（如溫度）或替換模型。\n",
    "\n",
    "1. configurable_fields 方法\n",
    "- 功能：允許你在運行時配置特定的 Runnable 字段。\n",
    "- 作用：這個方法與 bind() 方法類似，但不同之處在於 bind() 是預先設置參數，而 configurable_fields 則是在運行時動態設置參數。\n",
    "- 應用場景：當你需要在鏈中某一步驟根據需要臨時調整參數時，這個方法特別有用。例如，你可以在運行過程中調整 LLM 的溫度參數，而無需重新定義整個鏈。\n",
    "\n",
    "2. configurable_alternatives 方法\n",
    "- 功能：允許你列出任一 Runnable 的替代方案，並在運行時替換為這些指定的替代方案。\n",
    "- 作用：這個方法讓你能夠為任意步驟設置不同的實現方式，提供靈活性以便用戶在運行時選擇不同的替代項目。\n",
    "- 應用場景：當你想在運行中提供不同模型或方法選擇時，此方法非常實用。例如，根據用戶的輸入，你可以在某一步驟中選擇使用不同的 LLM 模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "簡單來說，可以在一個 chain 中保持輸入參數的彈性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurable Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! How about the number 27?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bfc6a7dc2', 'finish_reason': 'stop', 'logprobs': None}, id='run-f307cfe4-cc44-4738-aa41-614a14cb2c11-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    temperature=0, model=\"gpt-4o-mini\", api_key=OPENAI_KEY\n",
    ").configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM Temperature\",\n",
    "        description=\"The temperature of the LLM\",\n",
    "    )  # 指定一個 ConfigurableField為 'llm_temperature'\n",
    ")\n",
    "\n",
    "model.invoke(\"pick a random number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Let's go with the number 27.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-828ee339-f5bf-4d7d-aa4b-2dd2356ac966-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"pick a random number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 LCEL 的 chain 中要如何做？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! How about the number 27?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 14, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-5210e799-da37-4e30-b717-0575947cae5c-0', usage_metadata={'input_tokens': 14, 'output_tokens': 9, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Pick a random number above {x}\")\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"x\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Let's go with the number 42.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 14, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_9b78b61c52', 'finish_reason': 'stop', 'logprobs': None}, id='run-7d96d87f-2245-4bb1-9218-b02beda3a026-0', usage_metadata={'input_tokens': 14, 'output_tokens': 10, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.with_config(configurable={\"llm_temperature\": 0.9}).invoke({\"x\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們今天要同時測試 LLM 不同的 temperature 以及不同的 prompt 呢？\n",
    "\n",
    "這時候可以用 Runnable.configurable_alternatives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_default: content='Why do bears have hairy coats?\\n\\nBecause they look silly in sweaters!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 13, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None} id='run-fb55ece1-62ab-4918-91c8-6553173d413e-0' usage_metadata={'input_tokens': 13, 'output_tokens': 14, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "result_creative: content='In the hush of the forest, where tall pines sway,  \\nThe bears roam freely, in wild ballet.  \\nWith fur like the shadows, and eyes like the night,  \\nThey wander through meadows, in soft morning light.  \\n\\nA lumbering giant, with grace in each stride,  \\nIn rivers they fish, with the current as guide.  \\nFrom mountain to valley, in seasons they play,  \\nEmbracing the rhythms of night and of day.  \\n\\nCubs tumble and frolic, a playful delight,  \\nWhile mothers keep watch, in the coolness of night.  \\nOh, creatures of wonder, so fierce yet so grand,  \\nIn the heart of the wild, forever you stand.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 13, 'total_tokens': 158, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_9b78b61c52', 'finish_reason': 'stop', 'logprobs': None} id='run-590db484-5e4b-4ee6-92e6-a524f4f605ee-0' usage_metadata={'input_tokens': 13, 'output_tokens': 145, 'total_tokens': 158, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "result_very_creative: content='In the heart of a vast and ancient forest, where the trees towered like giants and the rivers sang gentle lullabies, lived a family of bears: Mama Bear, Papa Bear, and their two cubs, Benny and Bella. The forest was alive with the sounds of rustling leaves, bubbling brooks, and the occasional call of a distant bird. For the bear family, every day was an adventure waiting to unfold.\\n\\nOne radiant morning, as the sun peeked through the branches, casting dappled shadows over the forest floor, Mama Bear announced, “Today, we are going on a picnic by the clearing at the riverbank!” The cubs leaped with joy, their tiny paws dancing on the ground.\\n\\nAs Mama Bear packed a basket filled with honeycomb, berries, and fresh fish, Papa Bear gathered soft moss and blankets for their comfy spot beside the water. Benny and Bella scampered around, chasing butterflies and marveling at the busy ants marching to and fro.\\n\\nFinally, with their picnic supplies ready, the family set off through the woods. The scent of wildflowers filled the air, and Benny couldn’t resist stopping to nibble on some sweet clover, while Bella attempted to climb a tree, her tiny claws grasping for a sturdy branch.\\n\\nWhen they reached the riverbank, the scenery took their breath away. Sunbeams danced on the water’s surface, and the sound of the river flowing over smooth stones created a melody that wrapped around them like a warm hug. They spread their blankets and settled down for their feast.\\n\\nAs they enjoyed their picnic, a sudden commotion broke their tranquility. A rustling in the bushes nearby caught their attention, and from the underbrush emerged a little rabbit, its fur a soft shade of gray. It looked frightened and lost.\\n\\n“Are you okay?” Bella asked, her concern evident.\\n\\nThe rabbit, quivering slightly, replied, “I was separated from my family while exploring. Now, I can’t find my way back home.”\\n\\nBenny frowned, his brow furrowing with determination. “Don’t worry! We’ll help you!” he declared, his cub instincts kicking in.\\n\\nMama Bear smiled warmly. “That’s right. The forest is vast, but together we can find your family.”\\n\\nSo, after quickly finishing their picnic, the bear family, along with their new friend, set off on a grand quest through the woodland. They ventured through thickets, leaped over streams, and climbed gentle hills. Along the way, they asked every creature they met—squirrels, birds, even a wise old turtle—if they had seen the rabbit’s family.\\n\\nAfter what felt like hours of searching, they finally reached a grove where the sound of distant hopping echoed through the trees. The rabbit’s ears perked up, and he bounded ahead with excitement. Mama Bear and Papa Bear followed closely, with Benny and Bella right behind.\\n\\nThey soon came upon a group of rabbits, their noses twitching with anticipation. The little rabbit called out, “There’s my family!” Before they could say another word, he dashed into the welcoming embrace of his kin. The joyous reunion warmed the hearts of the bear family.\\n\\nAs they watched, Benny nudged Bella. “Helping someone is the best adventure of all,” he whispered.\\n\\nMama Bear beamed with pride. “Indeed, it is, my little ones. Kindness is just as important as any wilderness journey.”\\n\\nWith the sun setting on the horizon, coloring the sky in hues of orange and pink, the bears made their way back home, their hearts light with the warmth of friendship. The forest around them felt even more alive, filled with the promise of tomorrow’s adventures.\\n\\nFrom that day on, the bear family and their rabbit friend met often by the riverbank, sharing picnics and stories, forever bound by the spirit of kindness they had fostered on that bright, adventurous day. The forest was no longer just home; it was a place of friendship, adventure, and love, where every creature, big or small, belonged.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 821, 'prompt_tokens': 13, 'total_tokens': 834, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None} id='run-e8327975-804f-4ad7-8251-aa82c59c1f1a-0' usage_metadata={'input_tokens': 13, 'output_tokens': 821, 'total_tokens': 834, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 創建基礎 LLM 但使其 temperature 可配置\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0, model=\"gpt-4o-mini\", api_key=OPENAI_KEY\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm_config\"),\n",
    "    default_key=\"default\",\n",
    "    # 不同的 temperature 配置\n",
    "    creative=ChatOpenAI(temperature=0.7, model=\"gpt-4o-mini\", api_key=OPENAI_KEY),\n",
    "    very_creative=ChatOpenAI(temperature=1.0, model=\"gpt-4o-mini\", api_key=OPENAI_KEY),\n",
    "    conservative=ChatOpenAI(temperature=0.2, model=\"gpt-4o-mini\", api_key=OPENAI_KEY),\n",
    ")\n",
    "\n",
    "# 創建可配置的 prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Tell me a joke about {topic}\"\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"prompt\"),\n",
    "    default_key=\"joke\",\n",
    "    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),\n",
    "    story=PromptTemplate.from_template(\"Write a short story about {topic}\"),\n",
    ")\n",
    "\n",
    "# 創建鏈\n",
    "chain = prompt | llm\n",
    "\n",
    "# 使用預設 temperature (0) 講笑話\n",
    "result_default = chain.invoke({\"topic\": \"bears\"})\n",
    "\n",
    "# 使用較高 temperature (0.7) 寫詩\n",
    "result_creative = chain.with_config(\n",
    "    configurable={\"prompt\": \"poem\", \"llm_config\": \"creative\"}\n",
    ").invoke({\"topic\": \"bears\"})\n",
    "\n",
    "# 使用最高 temperature (1.0) 寫故事\n",
    "result_very_creative = chain.with_config(\n",
    "    configurable={\"prompt\": \"story\", \"llm_config\": \"very_creative\"}\n",
    ").invoke({\"topic\": \"bears\"})\n",
    "\n",
    "\n",
    "# 顯示結果\n",
    "print(f\"result_default: {result_default}\")\n",
    "print(f\"result_creative: {result_creative}\")\n",
    "print(f\"result_very_creative: {result_very_creative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當我們建構好一個 chain 時，會需要檢查其中的 Components 或是畫出路徑圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_chain.get_graph(): \n",
      "Graph(nodes={'828769e8c9cf43fa9509d0be409bbd14': Node(id='828769e8c9cf43fa9509d0be409bbd14', name='Parallel<question>Input', data=<class 'langchain_core.utils.pydantic.RunnableParallel<question>Input'>, metadata=None), '3d5a51a8e5584e9ca40b4e1df3aa5844': Node(id='3d5a51a8e5584e9ca40b4e1df3aa5844', name='Parallel<question>Output', data=<class 'langchain_core.utils.pydantic.RunnableParallel<question>Output'>, metadata=None), '475b257b2f844fe48030125adf651076': Node(id='475b257b2f844fe48030125adf651076', name='ChatPromptTemplate', data=ChatPromptTemplate(input_variables=['question'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x11c401d00>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text).\"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), metadata=None), '932dff6126a64765a0c7b7d8d13f2bbd': Node(id='932dff6126a64765a0c7b7d8d13f2bbd', name='ChatOpenAI', data=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x168995dd0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1689ac150>, root_client=<openai.OpenAI object at 0x168929cd0>, root_async_client=<openai.AsyncOpenAI object at 0x168996350>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), metadata={}), 'affd493729544c7fb73ef41483d469ed': Node(id='affd493729544c7fb73ef41483d469ed', name='StrOutputParser', data=StrOutputParser(), metadata=None), 'defe6e5494004e4692a852ab4fadcba4': Node(id='defe6e5494004e4692a852ab4fadcba4', name='Passthrough', data=RunnablePassthrough(), metadata=None), '794bc88212244ecebffaead1a65eeaa4': Node(id='794bc88212244ecebffaead1a65eeaa4', name='Parallel<context>Input', data=<class 'langchain_core.utils.pydantic.RunnableParallel<context>Input'>, metadata=None), '31f26c8884c24302b80173e41b617235': Node(id='31f26c8884c24302b80173e41b617235', name='Parallel<context>Output', data=<class 'langchain_core.utils.pydantic.RunnableParallel<context>Output'>, metadata=None), 'c1e4df89a78a45619ce704470bc3be05': Node(id='c1e4df89a78a45619ce704470bc3be05', name='fake_retriever', data=RunnableLambda(fake_retriever), metadata=None), '110ac1d1f5fd4dc385ae1f12dc18fc60': Node(id='110ac1d1f5fd4dc385ae1f12dc18fc60', name='Passthrough', data=RunnablePassthrough(), metadata=None), 'e9cdf455c93f4364af7203728ef3bbbf': Node(id='e9cdf455c93f4364af7203728ef3bbbf', name='ChatPromptTemplate', data=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer the user question given the following context:\\n\\n{context}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), metadata=None), '3b68d2ecd9c34f7c92a2669679547644': Node(id='3b68d2ecd9c34f7c92a2669679547644', name='ChatOpenAI', data=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x168995dd0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1689ac150>, root_client=<openai.OpenAI object at 0x168929cd0>, root_async_client=<openai.AsyncOpenAI object at 0x168996350>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), metadata={}), 'e354e3ef4d294bd1866fd5289ff79ba1': Node(id='e354e3ef4d294bd1866fd5289ff79ba1', name='StrOutputParser', data=StrOutputParser(), metadata=None), '1c9d7ee7c381411285afca0ccce195fa': Node(id='1c9d7ee7c381411285afca0ccce195fa', name='StrOutputParserOutput', data=<class 'langchain_core.output_parsers.string.StrOutputParserOutput'>, metadata=None)}, edges=[Edge(source='475b257b2f844fe48030125adf651076', target='932dff6126a64765a0c7b7d8d13f2bbd', data=None, conditional=False), Edge(source='932dff6126a64765a0c7b7d8d13f2bbd', target='affd493729544c7fb73ef41483d469ed', data=None, conditional=False), Edge(source='828769e8c9cf43fa9509d0be409bbd14', target='475b257b2f844fe48030125adf651076', data=None, conditional=False), Edge(source='affd493729544c7fb73ef41483d469ed', target='3d5a51a8e5584e9ca40b4e1df3aa5844', data=None, conditional=False), Edge(source='828769e8c9cf43fa9509d0be409bbd14', target='defe6e5494004e4692a852ab4fadcba4', data=None, conditional=False), Edge(source='defe6e5494004e4692a852ab4fadcba4', target='3d5a51a8e5584e9ca40b4e1df3aa5844', data=None, conditional=False), Edge(source='794bc88212244ecebffaead1a65eeaa4', target='c1e4df89a78a45619ce704470bc3be05', data=None, conditional=False), Edge(source='c1e4df89a78a45619ce704470bc3be05', target='31f26c8884c24302b80173e41b617235', data=None, conditional=False), Edge(source='794bc88212244ecebffaead1a65eeaa4', target='110ac1d1f5fd4dc385ae1f12dc18fc60', data=None, conditional=False), Edge(source='110ac1d1f5fd4dc385ae1f12dc18fc60', target='31f26c8884c24302b80173e41b617235', data=None, conditional=False), Edge(source='3d5a51a8e5584e9ca40b4e1df3aa5844', target='794bc88212244ecebffaead1a65eeaa4', data=None, conditional=False), Edge(source='31f26c8884c24302b80173e41b617235', target='e9cdf455c93f4364af7203728ef3bbbf', data=None, conditional=False), Edge(source='e9cdf455c93f4364af7203728ef3bbbf', target='3b68d2ecd9c34f7c92a2669679547644', data=None, conditional=False), Edge(source='e354e3ef4d294bd1866fd5289ff79ba1', target='1c9d7ee7c381411285afca0ccce195fa', data=None, conditional=False), Edge(source='3b68d2ecd9c34f7c92a2669679547644', target='e354e3ef4d294bd1866fd5289ff79ba1', data=None, conditional=False)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 取得 graph 表示\n",
    "print(f\"full_chain.get_graph(): \\n{full_chain.get_graph()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                +-------------------------+             \n",
      "                | Parallel<question>Input |             \n",
      "                +-------------------------+             \n",
      "                   ***               ***                \n",
      "                ***                     ***             \n",
      "              **                           ***          \n",
      "+--------------------+                        **        \n",
      "| ChatPromptTemplate |                         *        \n",
      "+--------------------+                         *        \n",
      "           *                                   *        \n",
      "           *                                   *        \n",
      "           *                                   *        \n",
      "    +------------+                             *        \n",
      "    | ChatOpenAI |                             *        \n",
      "    +------------+                             *        \n",
      "           *                                   *        \n",
      "           *                                   *        \n",
      "           *                                   *        \n",
      "  +-----------------+                   +-------------+ \n",
      "  | StrOutputParser |                   | Passthrough | \n",
      "  +-----------------+                   +-------------+ \n",
      "                   ***               ***                \n",
      "                      ***         ***                   \n",
      "                         **     **                      \n",
      "               +--------------------------+             \n",
      "               | Parallel<question>Output |             \n",
      "               +--------------------------+             \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                +------------------------+              \n",
      "                | Parallel<context>Input |              \n",
      "                +------------------------+              \n",
      "                    ***             ***                 \n",
      "                  **                   **               \n",
      "                **                       **             \n",
      "     +----------------+              +-------------+    \n",
      "     | fake_retriever |              | Passthrough |    \n",
      "     +----------------+              +-------------+    \n",
      "                    ***             ***                 \n",
      "                       **         **                    \n",
      "                         **     **                      \n",
      "                +-------------------------+             \n",
      "                | Parallel<context>Output |             \n",
      "                +-------------------------+             \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                  +--------------------+                \n",
      "                  | ChatPromptTemplate |                \n",
      "                  +--------------------+                \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                      +------------+                    \n",
      "                      | ChatOpenAI |                    \n",
      "                      +------------+                    \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                    +-----------------+                 \n",
      "                    | StrOutputParser |                 \n",
      "                    +-----------------+                 \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                +-----------------------+               \n",
      "                | StrOutputParserOutput |               \n",
      "                +-----------------------+               \n"
     ]
    }
   ],
   "source": [
    "# 視覺化圖\n",
    "full_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatPromptTemplate(input_variables=['question'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x11c401d00>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text).\"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]),\n",
       " ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer the user question given the following context:\\n\\n{context}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with_fallbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們還可以為整個序列創建備援機制，這些備援方案本身也是序列。以下是如何使用兩個不同的模型設置這樣的備援機制：ChatOpenAI 和普通的 OpenAI（不是聊天模型）。由於 OpenAI 不是聊天模型，因此你很可能需要使用不同的提示模板。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To get to the shell station! That’s a clever twist on a classic joke! Your sense of humor is truly delightful!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 29, 'total_tokens': 54, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-b9da81df-e31e-4d7b-a9db-d0d4f45a9849-0', usage_metadata={'input_tokens': 29, 'output_tokens': 25, 'total_tokens': 54, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create a chain with a ChatModel\n",
    "# We add in a string output parser here so the outputs between the two are the same type\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Now lets create a chain with the normal OpenAI model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# 創建一個 error 的 chain\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a nice assistant who always includes a compliment in your response\",\n",
    "        ),\n",
    "        (\"human\", \"Why did the {animal} cross the road\"),\n",
    "    ]\n",
    ")\n",
    "chat_model = ChatOpenAI(model=\"gpt-fake\", api_key=OPENAI_KEY) # 這裡一定會出錯．因為這個 model 不存在\n",
    "bad_chain = chat_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "\n",
    "# 創建一個正確的 chain\n",
    "prompt_template = \"\"\"Instructions: You should always include a compliment in your response.\n",
    "\n",
    "Question: Why did the {animal} cross the road?\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "good_chain = prompt | llm\n",
    "\n",
    "\n",
    "# 我們可以預期 bad_chain 會出錯，這時候我們可以使用 with_fallbacks 來設定 fallback chain\n",
    "chain = bad_chain.with_fallbacks([good_chain])\n",
    "chain.invoke({\"animal\": \"turtle\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用語言模型時，經常會遇到底層 API 帶來的問題，例如速率限制或服務中斷。因此，當你將 LLM 應用投入生產時，保障其穩定性變得至關重要。為此，引入了「備援機制（fallbacks）」的概念。\n",
    "\n",
    "請參考：https://python.langchain.com/docs/how_to/fallbacks/#fallback-for-llm-api-errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也不一定是只有「出錯」，也可以是「不符合預期」，這時候也可以使用 with_fallbacks 來設定 fallback chain\n",
    "\n",
    "例如我們可以設定當回答不出來時，可以進階使用更好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994-01-30 18:30:00\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)\"\n",
    ")\n",
    "\n",
    "openai_35 = (\n",
    "    ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=OPENAI_KEY) | DatetimeOutputParser()\n",
    ")\n",
    "openai_4 = ChatOpenAI(model=\"gpt-4o\", api_key=OPENAI_KEY) | DatetimeOutputParser()\n",
    "\n",
    "only_35 = prompt | openai_35\n",
    "fallback_4 = prompt | openai_35.with_fallbacks([openai_4])\n",
    "\n",
    "try:\n",
    "    print(only_35.invoke({\"event\": \"the superbowl in 1994\"}))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994-01-30 18:30:00\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(fallback_4.invoke({\"event\": \"the superbowl in 1994\"}))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡只是一個範例，雖然兩個模型都能夠回答的出來，但如果放到更複雜與困難的問題，這個方式就很管用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass Secret through '__'(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在運行時，我們可以使用 RunnableConfig 將機密信息傳遞給 Runnables。具體來說，可以在可配置字段前加上 __ 前綴來傳遞機密信息。這樣可以確保這些機密不會在調用過程中被追踪到。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def foo(x: int, config: RunnableConfig) -> int:\n",
    "    \"\"\"Sum x and a secret int\"\"\"\n",
    "    return x + config[\"configurable\"][\"__top_secret_int\"]\n",
    "\n",
    "\n",
    "foo.invoke({\"x\": 5}, {\"configurable\": {\"__top_secret_int\": 2, \"traced_key\": \"bar\"}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
